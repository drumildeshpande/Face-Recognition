{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FREQ_DIV = 5   \n",
    "RESIZE_FACTOR = 4\n",
    "NUM_TRAINING = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TrainEigenFaces:\n",
    "    def __init__(self):\n",
    "        cascPath = \"haarcascade_frontalface_default.xml\"\n",
    "        self.face_cascade = cv2.CascadeClassifier(cascPath)\n",
    "        self.face_dir = 'train_data'\n",
    "        self.face_name = sys.argv[1]\n",
    "        self.path = os.path.join(self.face_dir, self.face_name)\n",
    "        if not os.path.isdir(self.path):\n",
    "            os.mkdir(self.path)\n",
    "        self.model = cv2.createEigenFaceRecognizer()\n",
    "        self.count_captures = 0\n",
    "        self.count_timer = 0\n",
    "\n",
    "    def capture_training_images(self):\n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "        while True:\n",
    "            self.count_timer += 1\n",
    "            ret, frame = video_capture.read()\n",
    "            inImg = np.array(frame)\n",
    "            outImg = self.process_image(inImg)\n",
    "            cv2.imshow('Video', outImg)\n",
    "\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                video_capture.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                return\n",
    "\n",
    "    def process_image(self, inImg):\n",
    "        frame = cv2.flip(inImg,1)\n",
    "        resized_width, resized_height = (112, 92)        \n",
    "        if self.count_captures < NUM_TRAINING:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "            gray_resized = cv2.resize(gray, (gray.shape[1]/RESIZE_FACTOR, gray.shape[0]/RESIZE_FACTOR))        \n",
    "            faces = self.face_cascade.detectMultiScale(\n",
    "                gray_resized,\n",
    "                scaleFactor=1.1,\n",
    "                minNeighbors=5,\n",
    "                minSize=(30, 30),\n",
    "                flags=cv2.cv.CV_HAAR_SCALE_IMAGE\n",
    "                )\n",
    "            if len(faces) > 0:\n",
    "                areas = []\n",
    "                for (x, y, w, h) in faces: \n",
    "                    areas.append(w*h)\n",
    "                max_area, idx = max([(val,idx) for idx,val in enumerate(areas)])\n",
    "                face_sel = faces[idx]\n",
    "            \n",
    "                x = face_sel[0] * RESIZE_FACTOR\n",
    "                y = face_sel[1] * RESIZE_FACTOR\n",
    "                w = face_sel[2] * RESIZE_FACTOR\n",
    "                h = face_sel[3] * RESIZE_FACTOR\n",
    "\n",
    "                face = gray[y:y+h, x:x+w]\n",
    "                face_resized = cv2.resize(face, (resized_width, resized_height))\n",
    "                img_no = sorted([int(fn[:fn.find('.')]) for fn in os.listdir(self.path) if fn[0]!='.' ]+[0])[-1] + 1\n",
    "                \n",
    "                if self.count_timer%FREQ_DIV == 0:\n",
    "                    cv2.imwrite('%s/%s.png' % (self.path, img_no), face_resized)\n",
    "                    self.count_captures += 1\n",
    "                    print \"Captured image: \", self.count_captures\n",
    "\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "                cv2.putText(frame, self.face_name, (x - 10, y - 10), cv2.FONT_HERSHEY_PLAIN, 1,(0, 255, 0))\n",
    "        elif self.count_captures == NUM_TRAINING:\n",
    "            print \"Training data captured. Press 'q' to exit.\"\n",
    "            self.count_captures += 1\n",
    "\n",
    "        return frame           \n",
    "\n",
    "\n",
    "    def eigen_train_data(self):\n",
    "        imgs = []\n",
    "        tags = []\n",
    "        index = 0\n",
    "\n",
    "        for (subdirs, dirs, files) in os.walk(self.face_dir):\n",
    "            for subdir in dirs:\n",
    "                img_path = os.path.join(self.face_dir, subdir)\n",
    "                for fn in os.listdir(img_path):\n",
    "                    path = img_path + '/' + fn\n",
    "                    tag = index\n",
    "                    imgs.append(cv2.imread(path, 0))\n",
    "                    tags.append(int(tag))\n",
    "                index += 1\n",
    "        (imgs, tags) = [np.array(item) for item in [imgs, tags]]\n",
    "\n",
    "        self.model.train(imgs, tags)\n",
    "        self.model.save('trained_data_for_eigen.xml')\n",
    "        print \"Training completed successfully\"\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: 'face_data/-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-58576562fce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainEigenFaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_training_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigen_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Type in next user to train, or press Recognize\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0b02aa44d288>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateEigenFaceRecognizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_captures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: 'face_data/-f'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    trainer = TrainEigenFaces()\n",
    "    trainer.capture_training_images()\n",
    "    trainer.eigen_train_data()\n",
    "    print \"Type in next user to train, or press Recognize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
